<h2>Synopsis</h2>

<p>In the challenge <a href="https://www.kaggle.com/c/titanic">Titanic - Machine Learning from Disaster</a> from <a href="https://www.kaggle.com">Kaggle</a>, you need to predict of what kind of people were likely to survive the disaster or did not. In particular, they ask to apply the tools of machine learning to predict which passengers survived the tragedy.</p>

<p>I&#39;ve split this up into two seperate parts.</p>

<p><strong>Part 1 - Data Exploration and basic Model Building</strong><br/>
Part 2 - Creating own variables   </p>

<h2>Data Exploration</h2>

<p>I&#39;ve download the train and test data from <a href="https://www.kaggle.com/c/titanic/data">Kaggle</a>. At this page you could also find the variable descriptions.  </p>

<p>Import the training and testing set into R.</p>

<pre class="lang:r decode:true">train &lt;- read.csv(&quot;train.csv&quot;)
test &lt;- read.csv(&quot;test.csv&quot;)
</pre>

<p>Let&#39;s have a look at the data.</p>

<pre class="lang:r decode:true">summary(train)
</pre>

<pre><code>##   PassengerId       Survived          Pclass     
##  Min.   :  1.0   Min.   :0.0000   Min.   :1.000  
##  1st Qu.:223.5   1st Qu.:0.0000   1st Qu.:2.000  
##  Median :446.0   Median :0.0000   Median :3.000  
##  Mean   :446.0   Mean   :0.3838   Mean   :2.309  
##  3rd Qu.:668.5   3rd Qu.:1.0000   3rd Qu.:3.000  
##  Max.   :891.0   Max.   :1.0000   Max.   :3.000  
##                                                  
##                                     Name         Sex           Age       
##  Abbing, Mr. Anthony                  :  1   female:314   Min.   : 0.42  
##  Abbott, Mr. Rossmore Edward          :  1   male  :577   1st Qu.:20.12  
##  Abbott, Mrs. Stanton (Rosa Hunt)     :  1                Median :28.00  
##  Abelson, Mr. Samuel                  :  1                Mean   :29.70  
##  Abelson, Mrs. Samuel (Hannah Wizosky):  1                3rd Qu.:38.00  
##  Adahl, Mr. Mauritz Nils Martin       :  1                Max.   :80.00  
##  (Other)                              :885                NA&#39;s   :177    
##      SibSp           Parch             Ticket         Fare       
##  Min.   :0.000   Min.   :0.0000   1601    :  7   Min.   :  0.00  
##  1st Qu.:0.000   1st Qu.:0.0000   347082  :  7   1st Qu.:  7.91  
##  Median :0.000   Median :0.0000   CA. 2343:  7   Median : 14.45  
##  Mean   :0.523   Mean   :0.3816   3101295 :  6   Mean   : 32.20  
##  3rd Qu.:1.000   3rd Qu.:0.0000   347088  :  6   3rd Qu.: 31.00  
##  Max.   :8.000   Max.   :6.0000   CA 2144 :  6   Max.   :512.33  
##                                   (Other) :852                   
##          Cabin     Embarked
##             :687    :  2   
##  B96 B98    :  4   C:168   
##  C23 C25 C27:  4   Q: 77   
##  G6         :  4   S:644   
##  C22 C26    :  3           
##  D          :  3           
##  (Other)    :186
</code></pre>

<pre class="lang:r decode:true">head(train,2)
</pre>

<pre><code>##   PassengerId Survived Pclass
## 1           1        0      3
## 2           2        1      1
##                                                  Name    Sex Age SibSp
## 1                             Braund, Mr. Owen Harris   male  22     1
## 2 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female  38     1
##   Parch    Ticket    Fare Cabin Embarked
## 1     0 A/5 21171  7.2500              S
## 2     0  PC 17599 71.2833   C85        C
</code></pre>

<pre class="lang:r decode:true">head(test,2)
</pre>

<pre><code>##   PassengerId Pclass                             Name    Sex  Age SibSp
## 1         892      3                 Kelly, Mr. James   male 34.5     0
## 2         893      3 Wilkes, Mrs. James (Ellen Needs) female 47.0     1
##   Parch Ticket   Fare Cabin Embarked
## 1     0 330911 7.8292              Q
## 2     0 363272 7.0000              S
</code></pre>

<pre class="lang:r decode:true">dim(train)
</pre>

<pre><code>## [1] 891  12
</code></pre>

<pre class="lang:r decode:true">dim(test)
</pre>

<pre><code>## [1] 418  11
</code></pre>

<pre class="lang:r decode:true">str(train)
</pre>

<pre><code>## &#39;data.frame&#39;:    891 obs. of  12 variables:
##  $ PassengerId: int  1 2 3 4 5 6 7 8 9 10 ...
##  $ Survived   : int  0 1 1 1 0 0 0 0 1 1 ...
##  $ Pclass     : int  3 1 3 1 3 3 1 3 3 2 ...
##  $ Name       : Factor w/ 891 levels &quot;Abbing, Mr. Anthony&quot;,..: 109 191 358 277 16 559 520 629 417 581 ...
##  $ Sex        : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 1 1 1 2 2 2 2 1 1 ...
##  $ Age        : num  22 38 26 35 35 NA 54 2 27 14 ...
##  $ SibSp      : int  1 1 0 1 0 0 0 3 0 1 ...
##  $ Parch      : int  0 0 0 0 0 0 0 1 2 0 ...
##  $ Ticket     : Factor w/ 681 levels &quot;110152&quot;,&quot;110413&quot;,..: 524 597 670 50 473 276 86 396 345 133 ...
##  $ Fare       : num  7.25 71.28 7.92 53.1 8.05 ...
##  $ Cabin      : Factor w/ 148 levels &quot;&quot;,&quot;A10&quot;,&quot;A14&quot;,..: 1 83 1 57 1 1 131 1 1 1 ...
##  $ Embarked   : Factor w/ 4 levels &quot;&quot;,&quot;C&quot;,&quot;Q&quot;,&quot;S&quot;: 4 2 4 4 4 3 4 4 4 2 ...
</code></pre>

<pre class="lang:r decode:true">str(test)
</pre>

<pre><code>## &#39;data.frame&#39;:    418 obs. of  11 variables:
##  $ PassengerId: int  892 893 894 895 896 897 898 899 900 901 ...
##  $ Pclass     : int  3 3 2 3 3 3 3 2 3 3 ...
##  $ Name       : Factor w/ 418 levels &quot;Abbott, Master. Eugene Joseph&quot;,..: 210 409 273 414 182 370 85 58 5 104 ...
##  $ Sex        : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 1 2 2 1 2 1 2 1 2 ...
##  $ Age        : num  34.5 47 62 27 22 14 30 26 18 21 ...
##  $ SibSp      : int  0 1 0 0 1 0 0 1 0 2 ...
##  $ Parch      : int  0 0 0 0 1 0 0 1 0 0 ...
##  $ Ticket     : Factor w/ 363 levels &quot;110469&quot;,&quot;110489&quot;,..: 153 222 74 148 139 262 159 85 101 270 ...
##  $ Fare       : num  7.83 7 9.69 8.66 12.29 ...
##  $ Cabin      : Factor w/ 77 levels &quot;&quot;,&quot;A11&quot;,&quot;A18&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ Embarked   : Factor w/ 3 levels &quot;C&quot;,&quot;Q&quot;,&quot;S&quot;: 2 3 2 3 3 3 2 3 1 3 ...
</code></pre>

<p>The training set has <strong>891</strong> observations and <strong>12</strong> variables and the testing set has <strong>418</strong> observations and <strong>11</strong> variables. The traning set has <strong>1</strong> extra varible. Check which which one we are missing. I know we could see that in a very small dataset like this, but if its larger we want two compare them.</p>

<pre class="lang:r decode:true">colnames_check &lt;- colnames(train) %in% colnames(test)
colnames(train[colnames_check==FALSE])
</pre>

<pre><code>## [1] &quot;Survived&quot;
</code></pre>

<p>As we can see we are missing the <code>Survived</code> in the test set. Which is correct because thats our challenge, we must predict this by creating a model.</p>

<p>Let&#39;s look deeper into the training set, and check how many passengers that survived vs did not make it.</p>

<pre class="lang:r decode:true">table(train$Survived) 
</pre>

<pre><code>## 
##   0   1 
## 549 342
</code></pre>

<p>Hmm oke, of the <strong>891</strong> there are only <strong>342</strong> who survived it. Check also as proportions.</p>

<pre class="lang:r decode:true">prop.table(table(train$Survived))
</pre>

<pre><code>## 
##         0         1 
## 0.6161616 0.3838384
</code></pre>

<p>A little more than one-third of the passengers survived the disaster. Now see if there is a difference between males and females that survived vs males that passed away.</p>

<pre class="lang:r decode:true">table(train$Sex, train$Survived)
</pre>

<pre><code>##         
##            0   1
##   female  81 233
##   male   468 109
</code></pre>

<pre class="lang:r decode:true">prop.table(table(train$Sex, train$Survived),margin = 1)
</pre>

<pre><code>##         
##                  0         1
##   female 0.2579618 0.7420382
##   male   0.8110919 0.1889081
</code></pre>

<p>As we can see most of the female survived and most of the male did not make it.</p>

<h2>Model Building</h2>

<p>After doing some exploratory analysis of the data, let&#39;s do some first prediction before getting deeper into the data.</p>

<h3>First prediction - All Female Survived</h3>

<p>Create a copy of <code>test</code> to <code>test_female</code>, Initialize a <code>Survived</code> column to 0 and Set <code>Survived</code> to 1 if <code>Sex</code> equals &ldquo;female&rdquo;</p>

<pre class="lang:r decode:true">test_female &lt;- test
test_female$Survived &lt;- 0
test_female$Survived[test_female$Sex == &quot;female&quot;] &lt;- 1
</pre>

<p>Create a data frame with two columns: PassengerId &amp; Survived and write the solution away to a csv file.</p>

<pre class="lang:r decode:true">my_solution &lt;- data.frame(PassengerId = test_female$PassengerId, Survived = test_female$Survived)
write.csv(my_solution, file =  &quot;all_female.csv&quot;, row.names = FALSE)
</pre>

<p>That&#39;s our first submission to Kaggle and it&#39;s good for a score of <strong>0.76555</strong>. That&#39;s not so bad, but we want more!! :)</p>

<h3>Clean up the dataset</h3>

<p>Now we need to clean the dataset to create our models. Note that it is important to explore the data so that we understand what elements need to be cleaned.
For example we have noticed that there are missing values in the data set, especially in the <code>Age</code> column of the training set. Show which columns have missing values in the training and test set.</p>

<pre class="lang:r decode:true">colSums(is.na(train))
</pre>

<pre><code>## PassengerId    Survived      Pclass        Name         Sex         Age 
##           0           0           0           0           0         177 
##       SibSp       Parch      Ticket        Fare       Cabin    Embarked 
##           0           0           0           0           0           0
</code></pre>

<pre class="lang:r decode:true">colSums(is.na(test))
</pre>

<pre><code>## PassengerId      Pclass        Name         Sex         Age       SibSp 
##           0           0           0           0          86           0 
##       Parch      Ticket        Fare       Cabin    Embarked 
##           0           0           1           0           0
</code></pre>

<p>As we can see we have missing values in <code>Age</code> in the training set and <code>Age</code>, <code>Fare</code> in the test set.</p>

<p>To tackle the missing values I&#39;m going to predict the missing values with the full data set. First we need to combine the test and training set together.</p>

<pre class="lang:r decode:true">train2 &lt;- train
test2 &lt;- test
test2$Survived &lt;- NA
full &lt;- rbind(train2, test2)
</pre>

<p>First we tackle the missing <code>Fare</code>, because this is only one value. Let see in wich row it&#39;s missing. </p>

<pre class="lang:r decode:true">full[!complete.cases(full$Fare),]
</pre>

<pre><code>##      PassengerId Survived Pclass               Name  Sex  Age SibSp Parch
## 1044        1044       NA      3 Storey, Mr. Thomas male 60.5     0     0
##      Ticket Fare Cabin Embarked
## 1044   3701   NA              S
</code></pre>

<p>As we can see the passenger on row 1044 has an NA Fare value. Let&#39;s replace it with the median fare value.</p>

<pre class="lang:r decode:true">full$Fare[1044] &lt;- median(full$Fare, na.rm = TRUE)
</pre>

<p>How to fill in missing <code>Age</code> values? We make a prediction of a passengers Age using the other variables and a decision tree model. 
This time we give method = &ldquo;anova&rdquo; since you are predicting a continuous variable.</p>

<pre class="lang:r decode:true">library(rpart)
predicted_age &lt;- rpart(Age ~ Pclass + Sex + SibSp + Parch + Fare + Embarked,
                       data = full[!is.na(full$Age),], method = &quot;anova&quot;)
full$Age[is.na(full$Age)] &lt;- predict(predicted_age, full[is.na(full$Age),])
</pre>

<p>We know that the training set has 891 observations and the test set 418, we can split the data back into a train set and a test set. </p>

<pre class="lang:r decode:true">train2 &lt;- full[1:891,]
test2 &lt;- full[892:1309,]
</pre>

<h3>Build a Decision Tree with rpart</h3>

<p>Build the decision tree with rpart to predict <code>Survived</code> with the variables <code>Pclass</code>, <code>Sex</code>, <code>Age</code>, <code>SibSp</code>, <code>Parch</code>, <code>Fare</code> and <code>Embarked</code>.</p>

<pre class="lang:r decode:true">my_dt1 &lt;- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, 
                     data = train2, 
                     method = &quot;class&quot;)
</pre>

<p>Load in the packages to create a fancified visualized version of your tree.</p>

<pre class="lang:r decode:true">library(rattle)
library(rpart.plot)
library(RColorBrewer)
</pre>

<p>Visualize the decision tree using fancy tree of rpart.</p>

<pre class="lang:r decode:true">fancyRpartPlot(my_dt1)
</pre>

<p><img src="http://www.networkx.nl/wp-content/uploads/2016/02/my_dt1-1-2.png" alt="plot of chunk my_dt1"/> </p>

<p>From the top we can see that the node is voting 0, so at this level everyone would die. Below that we see that 62% of passengers die, while 38% survive (the most will die here that&#39;s why the node is voting that everyone die). If we go down to the male/female 81%/26% will die and 19%/74% will survive as the proportions exactly match those we find earlier. Let&#39;s see the proportions again rounded with two decimals.</p>

<pre class="lang:r decode:true">round(prop.table(table(train2$Survived)),2)
</pre>

<pre><code>## 
##    0    1 
## 0.62 0.38
</code></pre>

<pre class="lang:r decode:true">round(prop.table(table(train2$Sex, train2$Survived),margin = 1),2)
</pre>

<pre><code>##         
##             0    1
##   female 0.26 0.74
##   male   0.81 0.19
</code></pre>

<p>That are the same number&#39;s :)</p>

<p>Make the prediction using the test2 set.</p>

<pre class="lang:r decode:true">my_prediction &lt;- predict(my_dt1, newdata = test2, type = &quot;class&quot;)
</pre>

<p>Create a data frame with two columns: PassengerId &amp; Survived. Survived contains your predictions.</p>

<pre class="lang:r decode:true">my_solution &lt;- data.frame(PassengerId = test2$PassengerId, Survived = my_prediction)
</pre>

<p>Check that your data frame has 418 entries.</p>

<pre class="lang:r decode:true">nrow(my_solution)
</pre>

<pre><code>## [1] 418
</code></pre>

<p>Write your solution to a csv file with the name my_dt1.csv.</p>

<pre class="lang:r decode:true">write.csv(my_solution, file =  &quot;my_dt1.csv&quot;, row.names = FALSE)
</pre>

<p>This gives u a score of <strong>0.77512</strong>, this is a little better than our first submission.</p>

<p>Create a new decision tree <code>my_dt2</code> with some control aspects. The aspects are <code>cp</code> for splitting up of the decision tree stops and <code>minsplit</code> for the amount of observations in a bucket.</p>

<pre class="lang:r decode:true">my_dt2 &lt;- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, 
                       data = train2, 
                       method = &quot;class&quot;,
                       control = rpart.control(minsplit = 50, cp = 0))
</pre>

<p>Visualize your new decision tree.</p>

<pre class="lang:r decode:true">fancyRpartPlot(my_dt2)
</pre>

<p><img src="http://www.networkx.nl/wp-content/uploads/2016/02/my_dt2-1-2.png" alt="plot of chunk my_dt2"/> </p>

<p>Make the prediction using the <code>test2</code>, create the two column dataset, check the amount of rows and save it to <code>my_dt2.csv</code>.</p>

<pre class="lang:r decode:true">my_prediction &lt;- predict(my_dt2, newdata = test2, type = &quot;class&quot;)
my_solution &lt;- data.frame(PassengerId = test2$PassengerId, Survived = my_prediction)
nrow(my_solution)
</pre>

<pre><code>## [1] 418
</code></pre>

<pre class="lang:r decode:true">write.csv(my_solution, file =  &quot;my_dt2.csv&quot;, row.names = FALSE)
</pre>

<p>This will gives us a score of <strong>0.74163</strong>. Oke this is not an improvement.  </p>

<p>In part two I will create my own variables for making a model.</p>

